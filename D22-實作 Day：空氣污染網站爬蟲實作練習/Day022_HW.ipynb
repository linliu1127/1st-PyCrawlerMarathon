{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 空氣污染監測網 網路爬蟲實作練習\n",
    "\n",
    "\n",
    "* 能夠利用 selenium + BeautifulSoup 撰寫爬蟲，並存放到合適的資料結構\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業目標\n",
    "\n",
    "根據範例 ，完成以下問題：\n",
    "\n",
    "* ① 取出 台北市士林區 2018/01 – 2018/08 的 SO2 資料\n",
    "* ② 取出 台北市士林區 2018/01 – 2018/08 的 SO2、CO 資料\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ① 取出 台北市士林區 2018/01 – 2018/08 的 SO2 資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "SO2\n2018/01  1.80\n2018/02  1.90\n2018/03  2.20\n2018/04  2.30\n2018/05  3.10\n2018/06  2.70\n2018/07  2.20\n2018/08  2.40\n"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "import time\n",
    "import pandas as pd\n",
    "browser = webdriver.Chrome(executable_path=\"C:/Users/Username/Desktop/program/python/1st-PyCrawlerMarathon/chromedriver\")\n",
    "\n",
    "browser.get(\"http://taqm.epa.gov.tw/taqm/tw/MonthlyAverage.aspx\")\n",
    "browser.implicitly_wait(10)\n",
    "selectSite = Select(browser.find_element_by_id(\"ctl05_ddlSite\"))\n",
    "selectSite.select_by_value('11')\n",
    "selectYear = Select(browser.find_element_by_id(\"ctl05_ddlYear\"))\n",
    "selectYear.select_by_value('2018')\n",
    "\n",
    "browser.find_element_by_id('ctl05_btnQuery').click()\n",
    "time.sleep(3)\n",
    "html_source = browser.page_source\n",
    "dic={}\n",
    "soup = BeautifulSoup(html_source, 'html.parser')\n",
    "table = soup.find('table', class_='TABLE_G')\n",
    "\n",
    "for d in table.find_all('tr')[1:9]:\n",
    "    if len(d.find_all('td'))==5:\n",
    "        obs=d.find_all('td')[0].text\n",
    "        dic.setdefault(obs, {})\n",
    "        date=d.find_all('td')[2].text\n",
    "        data=d.find_all('td')[3].text\n",
    "        dic[obs][date]=data\n",
    "    if len(d.find_all('td'))==3:\n",
    "        date=d.find_all('td')[0].text\n",
    "        data=d.find_all('td')[1].text\n",
    "        dic[obs][date]=data\n",
    "d=pd.DataFrame(dic)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ② 取出 台北市士林區 2018/01 – 2018/08 的 SO2、CO 資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{'SO2': {'2018/01': '1.80', '2018/02': '1.90', '2018/03': '2.20', '2018/04': '2.30', '2018/05': '3.10', '2018/06': '2.70', '2018/07': '2.20', '2018/08': '2.40'}, 'CO': {'2018/01': '0.34', '2018/02': '0.44', '2018/03': '0.40', '2018/04': '0.38', '2018/05': '0.34', '2018/06': '0.29', '2018/07': '0.21', '2018/08': '0.30'}}\n          SO2    CO\n2018/01  1.80  0.34\n2018/02  1.90  0.44\n2018/03  2.20  0.40\n2018/04  2.30  0.38\n2018/05  3.10  0.34\n2018/06  2.70  0.29\n2018/07  2.20  0.21\n2018/08  2.40  0.30\n"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "import time\n",
    "import pandas as pd\n",
    "browser = webdriver.Chrome(executable_path=\"C:/Users/Username/Desktop/program/python/1st-PyCrawlerMarathon/chromedriver\")\n",
    "\n",
    "browser.get(\"http://taqm.epa.gov.tw/taqm/tw/MonthlyAverage.aspx\")\n",
    "browser.implicitly_wait(10)\n",
    "selectSite = Select(browser.find_element_by_id(\"ctl05_ddlSite\"))\n",
    "selectSite.select_by_value('11')\n",
    "selectYear = Select(browser.find_element_by_id(\"ctl05_ddlYear\"))\n",
    "selectYear.select_by_value('2018')\n",
    "\n",
    "browser.find_element_by_id('ctl05_btnQuery').click()\n",
    "time.sleep(3)\n",
    "html_source = browser.page_source\n",
    "soup = BeautifulSoup(html_source, 'html.parser')\n",
    "table = soup.find('table', class_='TABLE_G')\n",
    "dic1={}\n",
    "for d in table.find_all('tr')[1:9]:\n",
    "    if len(d.find_all('td'))==5:\n",
    "        obs=d.find_all('td')[0].text\n",
    "        dic1.setdefault(obs, {})\n",
    "        date=d.find_all('td')[2].text\n",
    "        data=d.find_all('td')[3].text\n",
    "        dic1[obs][date]=data\n",
    "    if len(d.find_all('td'))==3:\n",
    "        date=d.find_all('td')[0].text\n",
    "        data=d.find_all('td')[1].text\n",
    "        dic1[obs][date]=data\n",
    "for d in table.find_all('tr')[13:21]:\n",
    "    if len(d.find_all('td'))==5:\n",
    "        obs=d.find_all('td')[0].text\n",
    "        dic1.setdefault(obs, {})\n",
    "        date=d.find_all('td')[2].text\n",
    "        data=d.find_all('td')[3].text\n",
    "        dic1[obs][date]=data\n",
    "    if len(d.find_all('td'))==3:\n",
    "        date=d.find_all('td')[0].text\n",
    "        data=d.find_all('td')[1].text\n",
    "        dic1[obs][date]=data\n",
    "print(dic1)\n",
    "d1=pd.DataFrame(dic1)\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bita9990edcc1d34e4b85b4a57067bf3139"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}